{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiIDJ4Ah93pL"
   },
   "source": [
    "# Q1: Simple CNN network for PASCAL multi-label classification (20 points)\n",
    "Now let’s try to recognize some natural images. We provided some starter code for this task. The following steps will guide you through the process.\n",
    "\n",
    "\n",
    "## 1.1 Setup the dataset\n",
    "We start by modifying the code to read images from the PASCAL 2007 dataset. The important thing to note is that PASCAL can have multiple objects present in the same image. Hence, this is a multi-label classification problem, and will have to be tackled slightly differently.\n",
    "\n",
    "\n",
    "First, download the data. `cd` to a location where you can store 0.5GB of images. Then run:\n",
    "```\n",
    "wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "tar -xf VOCtrainval_06-Nov-2007.tar\n",
    "\n",
    "wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "tar -xf VOCtest_06-Nov-2007.tar\n",
    "cd VOCdevkit/VOC2007/\n",
    "```\n",
    "\n",
    "## 1.2 Write a dataloader with data augmentation (5 pts)\n",
    "**Dataloader** The first step is to write a [pytorch data loader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) which loads this PASCAL data. Browse the folders and files under `VOCdevkit` to understand the structure and labeling. Complete the functions `preload_anno` and `__getitem__` in `voc_dataset.py` according to the following instructions and the instructions in the code. More information about the dataset can be found [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/devkit_doc_07-Jun-2007.pdf). We will use data in ‘trainval’ for training and ‘test’ for testing.\n",
    "\n",
    "- `preload_anno`: This function will be called when the dataloader is initialized. We will load the annotations under folder `Annotations`. Each .xml file in the `Annotations` folder corresponds to the image with the same name under `JPEGImages`. In this function, we need to load `label` and `weight` vectors for each image according to the .xml file. \n",
    "- The labels should be 0 by default. Assign 1 for each class label in the .xml file. For example, in 000001.xml, the label vector should have 1s at the class indices correspond to 'dog' and 'person'. The rest of the vector should be 0.\n",
    "- The weights should be 1 by defatul. For each class label in the image, if 'difficult'=1 (which means it is ambiguous), we will assign 0 for the weight vector at this class index. This weight will be used when we calculate the test performance. We will not consider the ambiguous labels during testing.\n",
    "\n",
    "- `__getitem__`: This function will be called when the dataloader is called during training. It takes as input the index, and returns a tuple - `(image, label, weight)`. You need to load the image from the `JPEGImages` folder and load the corresponding label and weight using `self.anno_list`.\n",
    "\n",
    "\n",
    "**Data Augmentation** Modify `__getitem__` to randomly *augment* each datapoint using [TORCHVISION.TRANSFORMS](https://pytorch.org/vision/stable/transforms.html).  Make sure the data augmentation is only used for training data (based on self.split). Please describe what data augmentation you implement.\n",
    "\n",
    "- Before any augmentation, resize all the images based on `self.size`.\n",
    "\n",
    "* **Hint**: Since we are training a model from scratch on this small dataset, it is important to perform basic data augmentation to avoid overfitting. Add random crops and left-right flips when training, and do a center crop when testing, etc. As for natural images, another common practice is to subtract the mean values of RGB images from ImageNet dataset. The mean values for RGB images are: `[123.68, 116.78, 103.94]`. You may also rescale the images to `[−1, 1]`. There is no \"correct\" answer here! Feel free to search online about the data augmentation methods people usually use.\n",
    "\n",
    "### DESCRIBE YOUR AUGMENTATION PIPELINE HERE\n",
    "\n",
    "**Train Augmentations:**\n",
    "\n",
    "- I have gone with a widelly used setup of transforms\n",
    "    - RandomResizedCrop - This gets a random 224x224 patch from the input image.\n",
    "    - RandomHorizontalFlip - This flips the image along the vertical axis with a probability of 0.5\n",
    "    - ColorJitter - This introduces pertubations in the brightness, saturation and hue of the image.\n",
    "    - ToTensor - Converts it to a torch tensor and scales the values between 0 and 1.\n",
    "    - Normalize - Normalizes and recenters the RGB intensity values.\n",
    "- These augmentations bring enough variation to the original dataset/image so, the network doesn't overfit easily and incorporates a bit of robustness in its learning.\n",
    "<br>\n",
    "\n",
    "**Test Augmentations:**\n",
    "\n",
    "- The goal with the test augmentations is to ensure that the images are in the right size and scale. However, there shouldn't be any other pertubations or augmentations.\n",
    "  - Resize - The image is resized to a value slightly greater than 224 x 224 (~256)\n",
    "  - CenterCrop - A central patch of 224x224 is taken from this resized image.\n",
    "  - ToTensor - Converts it to a torch tensor and scales the values between 0 and 1.\n",
    "  - Normalize - Normalizes and recenters the RGB intensity values.\n",
    "\n",
    "\n",
    "## 1.3 Measure Performance (5 pts)\n",
    "To evaluate the trained model, we will use a standard metric for multi-label evaluation - [mean average precision (mAP)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html). Please implement `eval_dataset_map` in `utils.py` - this function will evaluate a model's map score using a given dataset object. You will need to make predictions on the given dataset with the model and call `compute_ap` to get average precision.\n",
    "\n",
    "\n",
    "Please describe how to compute AP for each class(not mAP).\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "For each class, the precision (Of all detected as true, how many are actually true - TP / (TP + FP)) and recall values (Of all the actual true, how many have been detected - TP / (TP + FN)) are caluculated at different threshold values. The threshold value here is the probability confidence at which a detection is considered true. These different precision and recall values are used to plot the Precision-Recall curve. The area under this curve gives the AP for that class.\n",
    "\n",
    "\n",
    "## 1.4 Let's Start Training! (5 pts)\n",
    "Fill out the loss function for multi-label classification in `trainer.py` and start training. In this question, you will use the model that you finished in the previous question (with proper non-linearities).\n",
    "\n",
    "Initialize a fresh model and optimizer. Then run your training code for 5 epochs and print the mAP on test set. The resulting mAP should be around 0.24. Make sure to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1645129818536,
     "user": {
      "displayName": "Punit Bhatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgrxmjUFMKUQAzHle30ZYoVyjo08hYnXCz-PrtI=s64",
      "userId": "00383454386425572934"
     },
     "user_tz": 300
    },
    "id": "fWUon_7PFd7V",
    "outputId": "91da4e36-309a-4786-f63a-849d06c1e9a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47555,
     "status": "ok",
     "timestamp": 1645129866084,
     "user": {
      "displayName": "Punit Bhatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgrxmjUFMKUQAzHle30ZYoVyjo08hYnXCz-PrtI=s64",
      "userId": "00383454386425572934"
     },
     "user_tz": 300
    },
    "id": "ZIwi3nZOGWzf",
    "outputId": "6b486f08-18a2-43d5-dc1b-3739d2e1105c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-17 20:30:18--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 460032000 (439M) [application/x-tar]\n",
      "Saving to: ‘VOCtrainval_06-Nov-2007.tar.3’\n",
      "\n",
      "VOCtrainval_06-Nov- 100%[===================>] 438.72M  22.5MB/s    in 21s     \n",
      "\n",
      "2022-02-17 20:30:39 (20.7 MB/s) - ‘VOCtrainval_06-Nov-2007.tar.3’ saved [460032000/460032000]\n",
      "\n",
      "--2022-02-17 20:30:42--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 451020800 (430M) [application/x-tar]\n",
      "Saving to: ‘VOCtest_06-Nov-2007.tar.3’\n",
      "\n",
      "VOCtest_06-Nov-2007 100%[===================>] 430.13M  22.2MB/s    in 21s     \n",
      "\n",
      "2022-02-17 20:31:03 (20.6 MB/s) - ‘VOCtest_06-Nov-2007.tar.3’ saved [451020800/451020800]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!tar -xf VOCtrainval_06-Nov-2007.tar\n",
    "\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar \n",
    "!tar -xf VOCtest_06-Nov-2007.tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1645129866084,
     "user": {
      "displayName": "Punit Bhatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgrxmjUFMKUQAzHle30ZYoVyjo08hYnXCz-PrtI=s64",
      "userId": "00383454386425572934"
     },
     "user_tz": 300
    },
    "id": "oxIz3jK9jAKe",
    "outputId": "10a21cb3-af7f-41a9-cea7-3bb678d9fabb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/spring22/16824/hw1\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/spring22/16824/hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1645130514363,
     "user": {
      "displayName": "Punit Bhatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgrxmjUFMKUQAzHle30ZYoVyjo08hYnXCz-PrtI=s64",
      "userId": "00383454386425572934"
     },
     "user_tz": 300
    },
    "id": "nIdxwDZ893pP",
    "outputId": "7db02983-b562-4c95-9fb0-3b5bc55ce5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "args.batch_size = 64\n",
      "args.device = cuda\n",
      "args.epochs = 5\n",
      "args.gamma = 0.7\n",
      "args.inp_size = 64\n",
      "args.log_every = 100\n",
      "args.lr = 0.001\n",
      "args.save_at_end = False\n",
      "args.save_freq = -1\n",
      "args.step_size = 2\n",
      "args.test_batch_size = 1000\n",
      "args.val_every = 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import trainer\n",
    "from utils import ARGS\n",
    "from simple_cnn import SimpleCNN\n",
    "from voc_dataset import VOCDataset\n",
    "\n",
    "# create hyperparameter argument class\n",
    "# Use image size of 64x64 in Q1. We will use a default size of 224x224 for the rest of the questions.\n",
    "args = ARGS(epochs=5, inp_size=64, lr=0.001, use_cuda=True, step_size=2)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255617,
     "status": "ok",
     "timestamp": 1645130770960,
     "user": {
      "displayName": "Punit Bhatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgrxmjUFMKUQAzHle30ZYoVyjo08hYnXCz-PrtI=s64",
      "userId": "00383454386425572934"
     },
     "user_tz": 300
    },
    "id": "qd80RKt393pQ",
    "outputId": "70e37cd0-02ba-4bb8-9ff1-a42897bb2f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0 (0%)]\tLoss: 0.696185\n",
      "Test MAP: 0.06912077030122211\n",
      "Train Epoch: 1 [100 (27%)]\tLoss: 0.225295\n",
      "Test MAP: 0.1667342101386896\n",
      "Train Epoch: 2 [200 (53%)]\tLoss: 0.213764\n",
      "Test MAP: 0.2125138354596569\n",
      "Train Epoch: 3 [300 (80%)]\tLoss: 0.190730\n",
      "Test MAP: 0.22802720779210411\n",
      "test map: 0.24274916326475351\n"
     ]
    }
   ],
   "source": [
    "# initializes the model\n",
    "model = SimpleCNN(num_classes=len(VOCDataset.CLASS_NAMES), inp_size=64, c_dim=3)\n",
    "# initializes Adam optimizer and simple StepLR scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "# trains model using your training code and reports test map\n",
    "test_ap, test_map = trainer.train(args,\n",
    "                                  model,\n",
    "                                  optimizer,\n",
    "                                  scheduler,\n",
    "                                  perform_transforms=False)\n",
    "print('test map:', test_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57ZO0KiD93pR"
   },
   "source": [
    "[TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) is an awesome visualization tool. It was firstly integrated in [TensorFlow](https://www.tensorflow.org/). It can be used to visualize training losses, network weights and other parameters.\n",
    "\n",
    "To use TensorBoard in Pytorch, there are two options: [TensorBoard in Pytorch](https://pytorch.org/docs/stable/tensorboard.html) (for Pytorch >= 1.1.0) or [TensorBoardX](https://github.com/lanpa/tensorboardX) - a third party library. Following these links to add code in `trainer.py` to visualize the testing MAP and training loss in Tensorboard. *You may have to reload the kernel for these changes to take effect.*\n",
    "\n",
    "Show clear screenshots of the learning curves of testing MAP and training loss for 5 epochs (batch size=20, learning rate=0.001). Please evaluate your model to calculate the MAP on the testing dataset every 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1173412,
     "status": "ok",
     "timestamp": 1645131996017,
     "user": {
      "displayName": "Punit Bhatt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgrxmjUFMKUQAzHle30ZYoVyjo08hYnXCz-PrtI=s64",
      "userId": "00383454386425572934"
     },
     "user_tz": 300
    },
    "id": "Yp9rJogY93pR",
    "outputId": "dbab5f52-df75-44d6-90f1-b339abc79067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0 (0%)]\tLoss: 0.684905\n",
      "Test MAP: 0.06769440696318658\n",
      "Train Epoch: 0 [100 (40%)]\tLoss: 0.211050\n",
      "Test MAP: 0.13373112827263375\n",
      "Train Epoch: 0 [200 (80%)]\tLoss: 0.204792\n",
      "Test MAP: 0.15840676214672533\n",
      "Train Epoch: 1 [300 (20%)]\tLoss: 0.210824\n",
      "Test MAP: 0.17806001499975693\n",
      "Train Epoch: 1 [400 (59%)]\tLoss: 0.219408\n",
      "Test MAP: 0.20322366567868588\n",
      "Train Epoch: 1 [500 (99%)]\tLoss: 0.208720\n",
      "Test MAP: 0.2152693991612653\n",
      "Train Epoch: 2 [600 (39%)]\tLoss: 0.196203\n",
      "Test MAP: 0.22711121966272732\n",
      "Train Epoch: 2 [700 (79%)]\tLoss: 0.237052\n",
      "Test MAP: 0.22825882652675858\n",
      "Train Epoch: 3 [800 (19%)]\tLoss: 0.177730\n",
      "Test MAP: 0.2415337900002846\n",
      "Train Epoch: 3 [900 (59%)]\tLoss: 0.196803\n",
      "Test MAP: 0.24234465702685964\n",
      "Train Epoch: 3 [1000 (98%)]\tLoss: 0.186077\n",
      "Test MAP: 0.24544857125469957\n",
      "Train Epoch: 4 [1100 (38%)]\tLoss: 0.154917\n",
      "Test MAP: 0.24847886105630432\n",
      "Train Epoch: 4 [1200 (78%)]\tLoss: 0.171937\n",
      "Test MAP: 0.25019227323943877\n",
      "test map: 0.25264846468947183\n"
     ]
    }
   ],
   "source": [
    "args = ARGS(epochs=5, batch_size=20, lr=0.001, inp_size=64)\n",
    "model = SimpleCNN(num_classes=len(VOCDataset.CLASS_NAMES), inp_size=64, c_dim=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "test_ap, test_map = trainer.train(args, model, optimizer, scheduler, model_name='model_20_001', perform_transforms=False)\n",
    "print('test map:', test_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVWvEiR993pS"
   },
   "source": [
    "**INSERT YOUR TENSORBOARD SCREENSHOTS HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv4mRfBxhn72"
   },
   "source": [
    "![image](images/simple_cnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "q1_simple_cnn_pascal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
