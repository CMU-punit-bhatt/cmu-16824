{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Lets go deeper! CaffeNet for PASCAL classification (20 pts)\n",
    "\n",
    "**Note:** You are encouraged to reuse code from the previous task. Finish Q1 if you haven't already!\n",
    "\n",
    "\n",
    "As you might have seen, the performance of the SimpleCNN model was pretty low for PASCAL. This is expected as PASCAL is much more complex than FASHION MNIST, and we need a much beefier model to handle it.\n",
    "\n",
    "In this task we will be constructing a variant of the [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) architecture, known as CaffeNet. If you are familiar with Caffe, a prototxt of the network is available [here](https://github.com/BVLC/caffe/blob/master/models/bvlc_reference_caffenet/train_val.prototxt). A visualization of the network is available [here](http://ethereon.github.io/netscope/#/preset/caffenet).\n",
    "\n",
    "\n",
    "## 2.1 Build CaffeNet (5 pts)\n",
    "Here is the exact model we want to build. In this task, `torchvision.models.xxx()` is NOT allowed. Define your own CaffeNet! We use the following operator notation for the architecture:\n",
    "1. Convolution: A convolution with kernel size $k$, stride $s$, output channels $n$, padding $p$ is represented as $conv(k, s, n, p)$.\n",
    "2. Max Pooling: A max pool operation with kernel size $k$, stride $s$ as $maxpool(k, s)$.\n",
    "3. Fully connected: For $n$ output units, $FC(n)$.\n",
    "4. ReLU: For rectified linear non-linearity $relu()$\n",
    "\n",
    "```\n",
    "ARCHITECTURE:\n",
    "-> image\n",
    "-> conv(11, 4, 96, ’VALID’)\n",
    "-> relu()\n",
    "-> max_pool(3, 2)\n",
    "-> conv(5, 1, 256, 'SAME')\n",
    "-> relu()\n",
    "-> max_pool(3, 2)\n",
    "-> conv(3, 1, 384, 'SAME')\n",
    "-> relu()\n",
    "-> conv(3, 1, 384, 'SAME')\n",
    "-> relu()\n",
    "-> conv(3, 1, 256, ’SAME’)\n",
    "-> relu()\n",
    "-> max_pool(3, 2)\n",
    "-> flatten()\n",
    "-> fully_connected(4096)\n",
    "-> relu()\n",
    "-> dropout(0.5)\n",
    "-> fully_connected(4096)\n",
    "-> relu()\n",
    "-> dropout(0.5)\n",
    "-> fully_connected(20)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import trainer\n",
    "from utils import ARGS\n",
    "from simple_cnn import SimpleCNN\n",
    "from voc_dataset import VOCDataset\n",
    "\n",
    "\n",
    "class CaffeNet(nn.Module):\n",
    "    def __init__(self, num_classes=20, inp_size=224, c_dim=3):\n",
    "        super().__init__()\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Save the Model (5 pts)\n",
    "Fill out `save_model()` in `trainer.py` to save the checkpoints of the model periodically. **You will need these models later.**\n",
    "\n",
    "\n",
    "## 2.3 Train and Test (5pts)\n",
    "Show clear screenshots of testing MAP and training loss for 50 epochs. The final MAP should be at least around 0.4. Please evaluate your model to calculate the MAP on the testing dataset every 250 iterations. Use the following hyperparamters:\n",
    "* batch_size=32\n",
    "* Adam optimizer with lr=0.0001\n",
    "\n",
    "**NOTE: SAVE AT LEAST 5 EVENLY SPACED CHECKPOINTS DURING TRAINING (1 at end)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ARGS(...)\n",
    "model = CaffeNet(...)\n",
    "optimizer = torch.optim.Adam(...)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(...)\n",
    "test_ap, test_map = trainer.train(args, model, optimizer, scheduler)\n",
    "print('test map:', test_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSERT YOUR TENSORBOARD SCREENSHOTS HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Visualizing: Conv-1 filters (5pts)\n",
    "Extract and compare the weights of conv1 filters at different stages of the training (at least from 5 different epochs).\n",
    "\n",
    "- Write a function to load your model checkpoints.\n",
    "- Get the weights for conv1 from the loaded model.\n",
    "- Visualize the weights using the following vis() function.\n",
    "\n",
    "Sometimes the filters all look very random and may not change too much across epochs. Don't worry! You will get full credits as long as the code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# This function plots all the filters in one image. \n",
    "def vis(conv1):\n",
    "    assert type(conv1) == np.ndarray\n",
    "    assert conv1.shape == (11, 11, 3, 96)\n",
    "    im = np.zeros((120, 120, 3))\n",
    "    step_size = 12\n",
    "    column = 0\n",
    "    row = 0\n",
    "    for k in range(conv1.shape[3]):\n",
    "        this_filter = conv1[:, :, :, k]\n",
    "        im[column*step_size:column*step_size+11, row*step_size:row*step_size+11, :] = this_filter\n",
    "        column = column + 1\n",
    "        if column == 10:\n",
    "            column = 0\n",
    "            row = row + 1\n",
    "    image = Image.fromarray(np.uint8((im-np.mean(im))/np.std(im)))\n",
    "    # image.show()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to get the conv1 filters for each epoch\n",
    "conv1s = [np.random.randn(11, 11, 3, 96)]\n",
    "\n",
    "# For each epoch, use vis() to visualize the filters.\n",
    "# Before passing the weights into vis(), make sure it is an numpy array with shape (11, 11, 3, 96).\n",
    "# You may need torch.permute to reorganize the dimensions.\n",
    "vis(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
